{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07b8a2-0123-4857-a280-d01d0335e27f",
   "metadata": {},
   "source": [
    "This Notebook contains a walk through:\n",
    "* The evaluation of BERT LLM model\n",
    "* Finetuning BERT with custom data (imdb) with different parameters\n",
    "* Compare the accuracy between the original model and after fine-tuning\n",
    "\n",
    "These tools and techiniques are used throughout the document:\n",
    "\n",
    "* LLM Model: **google-bert/bert-base-cased**\n",
    "* PEFT technique: **LoRA**\n",
    "* Evaluation approach: **Using accuracy metric**\n",
    "* Fine-tuning dataset: **stanfordnlp/imdb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "In this section, a pre-trained Hugging Face model is loaded and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b26e735a-101f-440e-98c3-403b1cdafb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You kight need to restart the kernel after this command to avoid errors when calling AutoModelForSequenceClassification\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d29dae-27b0-4ffa-b51e-baa9961dfcfc",
   "metadata": {},
   "source": [
    "## Prepare the Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9f8e5-75ec-47cd-b1b5-72ed07c0ff2d",
   "metadata": {},
   "source": [
    "### Load a pretrained HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce2c060-d8a5-4698-95b6-3162c357f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id=\"google-bert/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd52b1d-3019-4725-b98f-5f453467bab8",
   "metadata": {},
   "source": [
    "### Load and preprocess a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMDB contain train, test and unsupervised datasets with 25000, 25000 and 500000 samples respectively.\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_datasets = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_test_datasets = dataset[\"test\"].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d81fb54c-5169-4ad5-9382-64386c199fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_size = 6000\n",
    "eval_dataset_size = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36df5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_train_datasets.shuffle(seed=42).select(range(train_dataset_size))\n",
    "small_eval_dataset = tokenized_test_datasets.shuffle(seed=42).select(range(eval_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac9e9886-23e7-4742-8698-5e330adc1d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 3000\n",
      "}) Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 6000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(small_eval_dataset, small_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0ffeb-29cb-440e-9331-c05243756291",
   "metadata": {},
   "source": [
    "### Evaluate the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d20d27-a597-4ed8-bd69-136c035717b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a map between expected ids and labels\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "854dd356-65b0-48e8-948b-1c09230d83d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, \n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0c30a1f-ff16-4d72-a456-a1661f8b1a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Total # of params for the model google-bert/bert-base-cased: 108.31M\n"
     ]
    }
   ],
   "source": [
    "def count_params(model, is_human: bool = False):\n",
    "    params: int = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return f\"{params / 1e6:.2f}M\" if is_human else params\n",
    "\n",
    "print(model)\n",
    "print(\"Total # of params for the model {}: {}\".format(model_id, count_params(model, is_human=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af79e900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: A true stand out episode from season 1 is what Ice is.An artic location,claustrophobic conditions and a general feel of paranoia looming in the freezing air makes this is a must see episode from season one.The previous occupants of the artic station Mulder,Scully and four others go to have either killed each other or killed themselves.A virus is bringing out murderous aggression and is responsible for bringing out deadly paranoia and fear.Mulder and Scully actually begin to question each others sanity.Tension is that high.The writers have to receive great credit for creating that sort of scenario where the atmosphere is so tense Mulder and Scully come into conflict in such a direct manner,\n",
      "label:1 = POSITIVE\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Generate a random integer within the range of eval_dataset_size\n",
    "x = random.randint(0, eval_dataset_size)\n",
    "\n",
    "print(\"text: {},\\nlabel:{} = {}\".format(\n",
    "    small_eval_dataset[\"text\"][x],\n",
    "    small_eval_dataset[\"label\"][x],\n",
    "    id2label[small_eval_dataset[\"label\"][x]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62aa2908-c9e2-4f2a-bae7-cedd7aea53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use accuracy metric\n",
    "#Function inspired from https://huggingface.co/learn/nlp-course/en/chapter3/3#evaluation\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "742624c3-05dc-468a-9e8c-fe46fde89841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        \"evaluate_foundational_model\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16\n",
    "    ),\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecb8349e-b61c-4b2b-be95-dbc3a2bd95d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 02:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 1.31 s, total: 5.91 s\n",
      "Wall time: 2min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7342166304588318,\n",
       " 'eval_model_preparation_time': 0.0033,\n",
       " 'eval_accuracy': 0.49633333333333335,\n",
       " 'eval_runtime': 155.5398,\n",
       " 'eval_samples_per_second': 19.288,\n",
       " 'eval_steps_per_second': 1.209}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# Let's see the perfomance of the foundation model before any prior training\n",
    "trainer_foundation_eval=trainer.evaluate(eval_dataset=small_eval_dataset)\n",
    "trainer_foundation_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11655fb5-3f9d-477f-85a2-096631c205f8",
   "metadata": {},
   "source": [
    "###### **Without any fine tuning the model \"google-bert/bert-base-cased\" has an _accuracy_ of _0.4963_**\n",
    "###### **Not great, but we will improve it sustantialy with fione tuning in the next steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e894278-9998-4414-b4b9-ed306fee9ccb",
   "metadata": {},
   "source": [
    "### Saving the foundation model to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b41fd5e-b9be-4830-b2b8-d5203df2b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the foundational model to the local directory \"foundational_model/\" \n",
    "trainer.save_model(\"foundational_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "Create two PEFT models to test two different lora_config values and compare the results between the two. Save the PEFT model weights for each training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41ac75-cb10-4fa5-b7da-18b9884c87a0",
   "metadata": {},
   "source": [
    "### PEFT model (Same foundational model for the two PEFT configuraiotns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = model_id \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    peft_model_id,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db754fc-bb10-4657-8722-281a5d9b9ff3",
   "metadata": {},
   "source": [
    "### Create a PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e093652-ddea-489c-b930-00c91b8cb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an dictiopnary with two set of values for two trainings and performance comparaison\n",
    "peft_values= {\n",
    "    \"values1\": {\n",
    "        \"r\": 16,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"values2\": {\n",
    "        \"r\": 64,\n",
    "        \"lora_alpha\": 128,\n",
    "        \"lora_dropout\": 0.01,\n",
    "        \"bias\": \"none\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config1 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    r=peft_values[\"values1\"][\"r\"],\n",
    "    lora_alpha=peft_values[\"values1\"][\"lora_alpha\"],\n",
    "    lora_dropout=peft_values[\"values1\"][\"lora_dropout\"],\n",
    "    bias=peft_values[\"values1\"][\"bias\"],\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e13bde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 591,362 || all params: 108,903,172 || trainable%: 0.5430\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model1 = get_peft_model(model, lora_config1)\n",
    "lora_model1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed404b4-3103-45f7-9e7b-b66d997c7bce",
   "metadata": {},
   "source": [
    "**Here we can see the advantage of using PEFT fine tuning instead of training the whole model: only 0.543% of the 109 million parameters BERT has.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3e398-5fd0-41c1-ae5c-44f311a84d9d",
   "metadata": {},
   "source": [
    "### Train the PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b46862f9-750f-405e-af9e-728051e2b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_peft1 = TrainingArguments(\n",
    "    \"trainer_peft1_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4d4c908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 1:09:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.315898</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.288072</td>\n",
       "      <td>0.884333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>0.286446</td>\n",
       "      <td>0.882667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 2min 22s, total: 4min 50s\n",
      "Wall time: 1h 9min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1125, training_loss=0.38100865342881945, metrics={'train_runtime': 4163.4267, 'train_samples_per_second': 4.323, 'train_steps_per_second': 0.27, 'total_flos': 4768698949632000.0, 'train_loss': 0.38100865342881945, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer1 = Trainer(\n",
    "    model=lora_model1,\n",
    "    args=training_args_peft1,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.03 s, sys: 3.2 s, total: 9.23 s\n",
      "Wall time: 3min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.28644636273384094,\n",
       " 'eval_accuracy': 0.8826666666666667,\n",
       " 'eval_runtime': 237.0754,\n",
       " 'eval_samples_per_second': 12.654,\n",
       " 'eval_steps_per_second': 0.793,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer1_eval=trainer1.evaluate()\n",
    "trainer1_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9af57",
   "metadata": {},
   "source": [
    "##### **With fine tuning the model1 \"google-bert/bert-base-cased\" the _accuracy_ is now _0.882_ much better than the performance of the original foundational model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b2070",
   "metadata": {},
   "source": [
    "### Save the PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model1.save_pretrained(\"trainer_peft_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34d4ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6248\n",
      "drwxr-xr-x@  5 mk  staff      160 27 Dec 17:37 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  14 mk  staff      448  4 Jan 19:47 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "-rw-r--r--@  1 mk  staff     5101  4 Jan 19:47 README.md\n",
      "-rw-r--r--@  1 mk  staff  2372416  4 Jan 19:47 adapter_model.safetensors\n",
      "-rw-r--r--@  1 mk  staff      752  4 Jan 19:47 adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls -ltra trainer_peft_1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc0b88-6ae6-453f-b8ab-75e8d0ead40c",
   "metadata": {},
   "source": [
    "### Create PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a302c8ec-c10d-430d-89a2-7e7a917917ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config2 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    r=peft_values[\"values2\"][\"r\"],\n",
    "    lora_alpha=peft_values[\"values2\"][\"lora_alpha\"],\n",
    "    lora_dropout=peft_values[\"values2\"][\"lora_dropout\"],\n",
    "    bias=peft_values[\"values2\"][\"bias\"],\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23842433-d641-4c91-adcc-048bbb88bd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,360,834 || all params: 110,672,644 || trainable%: 2.1332\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model2 = get_peft_model(model, lora_config2)\n",
    "lora_model2.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc41b7-1d61-4d88-a2f9-ba536b3f1900",
   "metadata": {},
   "source": [
    "### Train PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab85654d-f9a8-49e7-8bd0-f402e2ab9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_peft2 = TrainingArguments(\n",
    "    \"trainer_peft2_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b26c5076-a39c-4dd1-b4ff-329c9d57f455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 1:15:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.275695</td>\n",
       "      <td>0.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>0.261908</td>\n",
       "      <td>0.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.260806</td>\n",
       "      <td>0.899333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 30s, sys: 3min, total: 5min 31s\n",
      "Wall time: 1h 16min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1125, training_loss=0.3054690348307292, metrics={'train_runtime': 4564.0093, 'train_samples_per_second': 3.944, 'train_steps_per_second': 0.246, 'total_flos': 4866543673344000.0, 'train_loss': 0.3054690348307292, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer2 = Trainer(\n",
    "    model=lora_model2,\n",
    "    args=training_args_peft2,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3c837e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 02:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.27 s, sys: 3.89 s, total: 10.2 s\n",
      "Wall time: 2min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.26080623269081116,\n",
       " 'eval_accuracy': 0.8993333333333333,\n",
       " 'eval_runtime': 169.1841,\n",
       " 'eval_samples_per_second': 17.732,\n",
       " 'eval_steps_per_second': 1.111,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer2_eval=trainer2.evaluate()\n",
    "trainer2_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf6c6f",
   "metadata": {},
   "source": [
    "**With fine tuning the model2 \"google-bert/bert-base-cased\" the _accuracy_ is now _0.899_ much better than the performance of the original foundational model and the PEFT1 model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2447c43",
   "metadata": {},
   "source": [
    "### Save the PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "280e874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model1.save_pretrained(\"trainer_peft_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01468279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18728\n",
      "drwxr-xr-x@  5 mk  staff      160 29 Dec 15:29 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  15 mk  staff      480  4 Jan 21:45 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "-rw-r--r--@  1 mk  staff     5101  4 Jan 21:47 README.md\n",
      "-rw-r--r--@  1 mk  staff  9450336  4 Jan 21:47 adapter_model.safetensors\n",
      "-rw-r--r--@  1 mk  staff      754  4 Jan 21:47 adapter_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -ltra trainer_peft_2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "Loading the PEFT model weights that has the best accuracy and evaluate the performance of the trained PEFT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefb3fe-6110-49f4-981c-7a87cb76b2e1",
   "metadata": {},
   "source": [
    "## Perform Inference Using the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679d481",
   "metadata": {},
   "source": [
    "### Load the saved PEFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d6d180",
   "metadata": {},
   "source": [
    "We load the best PEFT model of the two we created: \"trainer_peft_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "saved_model = AutoModelForSequenceClassification.from_pretrained(\"trainer_peft_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c08ece",
   "metadata": {},
   "source": [
    "### Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from eval_dataset: Maybe I'm biased because the F-16 is my favorite fighter aircraft - although the F-14 is probably second or third - but I liked this movie. The sequels (Iron Eagle II and III) don't measure up acting and plot wise, but the first one - along with Top Gun - have excellent flying and music, along with reasonable plots and acting. II and III clearly have much less of a \"flight budget\", but their main drawback is plot and acting. I suspect the relative fame and popularity of Iron Eagle compared to Top Gun is almost entirely a reflection of the fame and popularity of Jason Gedrick compared to Tom Cruise. Another plus (for me) is an all too brief appearance by Shawnee Smith. 7/10 \n",
      "\n",
      "label from eval_dataset:POSITIVE\n",
      "\n",
      "Predicted class: POSITIVE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "x = random.randint(0, eval_dataset_size)\n",
    "\n",
    "text_to_classify=small_eval_dataset[\"text\"][x]\n",
    "\n",
    "print(\"Text from eval_dataset: {} \\n\\nlabel from eval_dataset:{}\".format(\n",
    "    small_eval_dataset[\"text\"][x], \n",
    "    id2label[small_eval_dataset[\"label\"][x]]\n",
    "))\n",
    "\n",
    "\n",
    "def classify(text):\n",
    "    #Tokenize the text and return a PyTorch tensor\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    #Pass the tokinezed text to the model and get logits\n",
    "    with torch.no_grad():\n",
    "        outputs = saved_model(**inputs)\n",
    "    # Get the predicted class\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    print(f\"\\nPredicted class: {model.config.id2label[predictions.item()]} \\n\")\n",
    "\n",
    "\n",
    "classify(text_to_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03bc9e",
   "metadata": {},
   "source": [
    "**The inference classified the text as POSITIVE which matches the label in the dataset.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
